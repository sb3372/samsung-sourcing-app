import streamlit as st
from tavily import TavilyClient
from datetime import datetime, timedelta
import os
import json
import hashlib
import re

st.set_page_config(page_title="Samsung Strategic Sourcing Agent", layout="wide")

st.markdown("""
<style>
    .header-container {
        background: linear-gradient(90deg, #1428a0 0%, #0066ff 100%);
        padding: 2rem;
        border-radius: 10px;
        margin-bottom: 2rem;
    }
    .header-container h1 {
        color: white;
        margin: 0;
        font-size: 2.5rem;
    }
</style>
<div class="header-container">
    <h1>ğŸ›¡ï¸ Samsung ìœ ëŸ½ ì¡°ë‹¬ ì„¼í„° ì „ëµ ì¸í…”ë¦¬ì „ìŠ¤</h1>
</div>
""", unsafe_allow_html=True)

HISTORY_FILE = "article_history.json"

def load_history():
    if not os.path.exists(HISTORY_FILE):
        return {"urls": set(), "hashes": set()}
    try:
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            return {"urls": set(data.get("urls", [])), "hashes": set(data.get("hashes", []))}
    except:
        return {"urls": set(), "hashes": set()}

def save_history(history):
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump({
            "urls": list(history["urls"]),
            "hashes": list(history["hashes"])
        }, f, ensure_ascii=False, indent=2)

def get_hash(content):
    content = re.sub(r'\s+', ' ', content.lower())
    return hashlib.md5(content.encode()).hexdigest()

@st.cache_data
def translate_kr(text):
    try:
        from google_trans_new import google_translator
        return google_translator().translate(text, lang_src='en', lang_tgt='ko')
    except:
        return text

# ===== í•µì‹¬: ìŠ¤ë§ˆíŠ¸ ê¸°ì‚¬ í•„í„°ë§ =====
def is_high_quality_article(title, content, category_name):
    """
    ê¸°ì‚¬ í’ˆì§ˆ í‰ê°€ (0-100)
    - ë†’ì„ìˆ˜ë¡ ì¢‹ì€ ê¸°ì‚¬
    """
    score = 0
    
    # 1. ì½˜í…ì¸  ê¸¸ì´ (ìµœì†Œ 500ì)
    if len(content) < 500:
        return -1
    if len(content) > 300:
        score += 10
    
    # 2. êµ¬ì²´ì ì¸ ìˆ«ì/í†µê³„ í¬í•¨ (ë§¤ìš° ì¤‘ìš”!)
    numbers = re.findall(r'\b\d+(?:\.\d+)?[%M B billion million thousand]?\b', content)
    if len(numbers) >= 3:
        score += 30
    elif len(numbers) >= 1:
        score += 15
    
    # 3. ë‚ ì§œ ì •ë³´ í¬í•¨
    dates = re.findall(r'\b(202[4-6]|Q[1-4]|January|February|March|April|May|June|July|August|September|October|November|December)\b', content, re.IGNORECASE)
    if len(dates) > 0:
        score += 10
    
    # 4. ê¸°ì—…/ê¸°ê´€ëª… í¬í•¨ (ì‹ ë¢°ì„±)
    company_names = ['Samsung', 'ASML', 'TSMC', 'Intel', 'Qualcomm', 'Apple', 'EU', 'European', 'Germany', 'Netherlands', 'France']
    for company in company_names:
        if company.lower() in content.lower():
            score += 5
    
    # 5. ì¹´í…Œê³ ë¦¬ë³„ í‚¤ì›Œë“œ ë§¤ì¹­
    category_keywords = {
        "ì¡°ë‹¬ ë° ì†Œì¬": ["semiconductor", "chip", "price", "cost", "supply", "component", "memory", "processor"],
        "ê³µê¸‰ë§ ë° ë¬¼ë¥˜": ["port", "logistics", "disruption", "strike", "shipping", "lead time", "delivery"],
        "EU ê·œì œ ë° ì¤€ìˆ˜": ["regulation", "compliance", "CRA", "AI Act", "ESPR", "Digital Product Passport", "cybersecurity"],
        "í˜ì‹  ë° ìƒíƒœê³„": ["startup", "innovation", "6G", "robotics", "AI", "venture", "funding", "technology"],
        "Samsung í¬íŠ¸í´ë¦¬ì˜¤": ["Samsung", "telecommunication", "wearable", "consumer electronics", "device"]
    }
    
    keywords = category_keywords.get(category_name, [])
    keyword_count = sum(1 for kw in keywords if kw.lower() in content.lower())
    score += min(keyword_count * 5, 25)
    
    # 6. ì¤‘ìš” ë™ì‚¬/ì•¡ì…˜ í¬í•¨
    action_verbs = ['announce', 'launch', 'introduce', 'achieve', 'reach', 'surge', 'jump', 'grow', 'expand', 'partnership', 'strike', 'disrupt']
    action_count = sum(1 for verb in action_verbs if verb.lower() in content.lower())
    score += min(action_count * 3, 15)
    
    # 7. ì œëª©ê³¼ ë‚´ìš©ì˜ ì—°ê´€ì„±
    title_words = set(title.lower().split())
    content_first_300 = content[:300].lower()
    matching_words = sum(1 for word in title_words if word in content_first_300)
    if matching_words > len(title_words) * 0.3:
        score += 10
    
    return score

# ===== í•µì‹¬: ì •êµí•œ ê¸°ì‚¬ ìš”ì•½ =====
def summarize_article_korean(title, content):
    """
    ê¸°ì‚¬ë¥¼ 3ê°œì˜ í•µì‹¬ í¬ì¸íŠ¸ë¡œ ì •ë¦¬
    í¬ë§·:
    â–¡ ì œëª©
    - í¬ì¸íŠ¸1 (êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ í¬í•¨)
    Â· ì„¤ëª…1
    - í¬ì¸íŠ¸2
    Â· ì„¤ëª…2
    - í¬ì¸íŠ¸3
    Â· ì„¤ëª…3
    """
    
    # ë‹¨ê³„ 1: í•µì‹¬ ì •ë³´ ì¶”ì¶œ
    # 1) ìˆ«ìì™€ í•¨ê»˜ ë‚˜ì˜¤ëŠ” ë¬¸ì¥ ì°¾ê¸°
    sentences = re.split(r'(?<=[.!?])\s+', content)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
    
    # ê° ë¬¸ì¥ ì ìˆ˜ ê³„ì‚°
    def score_sentence(sent):
        score = 0
        
        # ìˆ«ì í¬í•¨ (ë§¤ìš° ì¤‘ìš”)
        numbers = re.findall(r'\d+\.?\d*[%M B]?', sent)
        if numbers:
            score += 50
        
        # ë¬¸ì¥ ê¸¸ì´ (ë„ˆë¬´ ì§§ìœ¼ë©´ ì•ˆë¨)
        if 20 < len(sent) < 200:
            score += 10
        
        # ì¤‘ìš” í‚¤ì›Œë“œ
        important_words = ['increase', 'growth', 'rise', 'jump', 'surge', 'reach', 'announce', 'launch', 'expand', 'partnership', 'challenge', 'threat', 'opportunity', 'market', 'new', 'first', 'breakthrough']
        for word in important_words:
            if word.lower() in sent.lower():
                score += 3
        
        # ì£¼ì–´-ë™ì‚¬-ëª©ì ì–´ êµ¬ì¡° (ì™„ì „í•œ ë¬¸ì¥)
        if re.search(r'\b[A-Z][a-z]+\s+(?:has|is|are|was|were|announced|said|reported)\b', sent):
            score += 5
        
        return score
    
    scored_sentences = [(sent, score_sentence(sent)) for sent in sentences]
    scored_sentences = sorted(scored_sentences, key=lambda x: x[1], reverse=True)
    
    # ìƒìœ„ 3ê°œ ì„ íƒ
    top_3 = [sent for sent, _ in scored_sentences[:3]]
    
    # ë‹¨ê³„ 2: í•œêµ­ì–´ë¡œ ë²ˆì—­ ë° ì •ë¦¬
    try:
        from google_trans_new import google_translator
        translator = google_translator()
        
        title_kr = translator.translate(title, lang_src='en', lang_tgt='ko')
        points_kr = [translator.translate(point, lang_src='en', lang_tgt='ko') for point in top_3]
    except:
        title_kr = title
        points_kr = top_3
    
    # ë‹¨ê³„ 3: ì„¸ë¶€ ì„¤ëª… ìƒì„±
    details = [
        "ì£¼ìš” ë‚´ìš© ë° ìˆ˜ì¹˜ë¥¼ ë°˜ì˜í•œ ë‚´ìš©ì…ë‹ˆë‹¤.",
        "ì‹œì¥ ë³€í™” ë° ì˜í–¥ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.",
        "í–¥í›„ ì „ë§ ë° ì˜ë¯¸ë¥¼ ë‹´ê³  ìˆìŠµë‹ˆë‹¤."
    ]
    
    # í¬ë§· ìƒì„±
    summary = f"â–¡ {title_kr}\n"
    for i, (point, detail) in enumerate(zip(points_kr, details)):
        # ë¶ˆë¦¿ í¬ì¸íŠ¸ ì •ë¦¬
        point_clean = re.sub(r'^[-â€¢*]\s*', '', point).strip()
        summary += f"- {point_clean}\n"
        summary += f"  Â· {detail}\n"
    
    return summary

# ===== ê¸°ì‚¬ ê²€ìƒ‰ ë° í•„í„°ë§ =====
def search_and_filter_articles(category_name, query, tavily_client, history, max_try=5):
    """
    1. Tavilyë¡œ ê²€ìƒ‰
    2. ê³ í’ˆì§ˆ ê¸°ì‚¬ë§Œ í•„í„°ë§
    3. ì¤‘ë³µ ì œê±°
    """
    
    try:
        results = tavily_client.search(
            query=query,
            search_depth="advanced",
            max_results=max_try,
            include_raw_content=True
        )
    except:
        return []
    
    filtered = []
    
    for res in results.get('results', []):
        url = res.get('url')
        title = res.get('title', '')
        content = res.get('content', '')
        
        # ê¸°ë³¸ ê²€ì¦
        if not url or not content or len(content) < 200:
            continue
        
        # ì¤‘ë³µ í™•ì¸
        if url in history["urls"]:
            continue
        
        content_hash = get_hash(content)
        if content_hash in history["hashes"]:
            continue
        
        # í’ˆì§ˆ í‰ê°€
        quality_score = is_high_quality_article(title, content, category_name)
        
        if quality_score < 40:  # ìµœì†Œ 40ì  ì´ìƒ
            continue
        
        filtered.append({
            "url": url,
            "title": title,
            "content": content,
            "quality": quality_score
        })
    
    # í’ˆì§ˆìˆœ ì •ë ¬
    filtered = sorted(filtered, key=lambda x: x['quality'], reverse=True)
    
    return filtered[:2]  # ìƒìœ„ 2ê°œë§Œ

# ===== UI =====
st.sidebar.header("âš™ï¸ ì„¤ì •")
tavily_key = st.sidebar.text_input("Tavily API Key", type="password")

history = load_history()
st.sidebar.markdown("---")
st.sidebar.metric("ì¶”ì ëœ ê¸°ì‚¬", len(history["urls"]))

if st.sidebar.button("ğŸ—‘ï¸ íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”"):
    if os.path.exists(HISTORY_FILE):
        os.remove(HISTORY_FILE)
    st.rerun()

st.markdown("---")

SEARCH_QUERIES = {
    "ì¡°ë‹¬ ë° ì†Œì¬": "Samsung Europe semiconductor chips supply price 2024 2025",
    "ê³µê¸‰ë§ ë° ë¬¼ë¥˜": "Europe port logistics disruption shipping 2024 2025",
    "EU ê·œì œ ë° ì¤€ìˆ˜": "EU CRA AI Act regulation compliance 2024 2025",
    "í˜ì‹  ë° ìƒíƒœê³„": "Europe 6G robotics startup innovation 2024 2025",
    "Samsung í¬íŠ¸í´ë¦¬ì˜¤": "Samsung Europe semiconductor innovation announcement 2024 2025"
}

if st.button("ğŸš€ ê³ í’ˆì§ˆ ê¸°ì‚¬ ê²€ìƒ‰ ì‹œì‘", use_container_width=True):
    if not tavily_key:
        st.error("âŒ Tavily API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        client = TavilyClient(api_key=tavily_key)
        history = load_history()
        
        pbar = st.progress(0)
        status = st.empty()
        
        all_articles = []
        category_results = {}
        
        for idx, (cat_name, query) in enumerate(SEARCH_QUERIES.items()):
            status.text(f"ğŸ” {cat_name} ê²€ìƒ‰ ì¤‘... (ê³ í’ˆì§ˆ ê¸°ì‚¬ë§Œ í•„í„°ë§)")
            
            # ê¸°ì‚¬ ê²€ìƒ‰ ë° í•„í„°ë§
            articles = search_and_filter_articles(cat_name, query, client, history, max_try=10)
            
            if articles:
                category_results[cat_name] = articles
                all_articles.extend(articles)
            
            pbar.progress((idx + 1) / len(SEARCH_QUERIES))
        
        pbar.empty()
        status.empty()
        
        # í†µê³„
        st.markdown("---")
        col1, col2, col3 = st.columns(3)
        col1.metric("ğŸ” ë°œê²¬ëœ ê¸°ì‚¬", len(all_articles))
        col2.metric("ğŸ“‚ ì¹´í…Œê³ ë¦¬", len(category_results))
        col3.metric("ğŸ’¾ ì´ ì¶”ì ", len(history["urls"]))
        
        st.markdown("---")
        
        if all_articles:
            article_num = 0
            
            for cat_name, articles in category_results.items():
                st.markdown(f"### ğŸ“‚ {cat_name}")
                
                for article in articles:
                    article_num += 1
                    
                    if article_num > 10:
                        break
                    
                    # ìš”ì•½ ìƒì„±
                    with st.spinner(f"ğŸ“ ê¸°ì‚¬ {article_num} ìš”ì•½ ì¤‘..."):
                        summary = summarize_article_korean(article['title'], article['content'])
                        
                        try:
                            title_kr = translate_kr(article['title'])
                        except:
                            title_kr = article['title']
                    
                    # í’ˆì§ˆ ì ìˆ˜ í‘œì‹œ
                    st.markdown(f"#### ğŸ“° {article_num}. {title_kr}")
                    st.caption(f"â­ í’ˆì§ˆì ìˆ˜: {article['quality']}/100 | ì›ë¬¸: {len(article['content'])}ì")
                    
                    # ìš”ì•½ í‘œì‹œ
                    st.markdown(summary)
                    
                    # ë²„íŠ¼
                    col1, col2 = st.columns([3, 1])
                    with col1:
                        st.markdown(f"[ğŸ“– ì „ì²´ ê¸°ì‚¬ ì½ê¸°]({article['url']})")
                    with col2:
                        if st.button("âœ… ì½ìŒ", key=f"btn_{article_num}"):
                            history["urls"].add(article['url'])
                            history["hashes"].add(get_hash(article['content']))
                            save_history(history)
                            st.success("ì™„ë£Œ!")
                    
                    st.divider()
        else:
            st.warning("âš ï¸ ê³ í’ˆì§ˆ ê¸°ì‚¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Tavily API í‚¤ë¥¼ í™•ì¸í•˜ì„¸ìš”.")
