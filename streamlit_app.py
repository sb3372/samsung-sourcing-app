import streamlit as st
from tavily import TavilyClient
from datetime import datetime, timedelta
import os
import json
import hashlib
import re

# ===== PAGE CONFIGURATION =====
st.set_page_config(
    page_title="Samsung Strategic Sourcing Agent",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ===== CUSTOM CSS =====
st.markdown("""
<style>
    :root {
        --samsung-blue: #1428a0;
        --samsung-accent: #0066ff;
        --dark-bg: #0f1419;
        --card-bg: #1a1f2e;
        --text-primary: #ffffff;
        --text-secondary: #b0b8c1;
    }
    
    .main {
        background: linear-gradient(135deg, #0f1419 0%, #1a1f2e 100%);
    }
    
    .header-container {
        background: linear-gradient(90deg, #1428a0 0%, #0066ff 100%);
        padding: 2rem;
        border-radius: 10px;
        margin-bottom: 2rem;
        box-shadow: 0 4px 15px rgba(20, 40, 160, 0.3);
    }
    
    .header-container h1 {
        color: white;
        margin: 0;
        font-size: 2.5rem;
        font-weight: 700;
    }
    
    .header-container p {
        color: rgba(255,255,255,0.9);
        margin: 0.5rem 0 0 0;
        font-size: 1rem;
    }
    
    .stButton>button {
        background: linear-gradient(90deg, #1428a0 0%, #0066ff 100%);
        color: white;
        border: none;
        font-weight: 600;
        padding: 0.6rem 1.5rem;
        border-radius: 6px;
    }
    
    [data-testid="stSidebar"] {
        background: #1a1f2e;
    }
    
    [data-testid="metric-container"] {
        background: #1a1f2e;
        border-left: 3px solid #0066ff;
    }
    
    a {
        color: #0066ff !important;
    }
</style>
""", unsafe_allow_html=True)

# ===== CONFIGURATION =====
LANGUAGES = {
    "English": "en",
    "German": "de",
    "French": "fr",
    "Spanish": "es",
}

CATEGORIES = {
    "ì¡°ë‹¬ ë° ì†Œì¬": {
        "emoji": "ğŸ’°",
        "queries": {
            "en": "semiconductor price volatility Europe 2024 2025",
            "de": "Halbleiter Preise Europa",
            "fr": "prix semiconducteur Europe",
            "es": "precios semiconductores Europa",
        },
    },
    "ê³µê¸‰ë§ ë° ë¬¼ë¥˜": {
        "emoji": "ğŸš¢",
        "queries": {
            "en": "logistics disruption Europe port strikes 2024",
            "de": "Logistik StÃ¶rungen Europa",
            "fr": "perturbations logistiques Europe",
            "es": "disrupciones logÃ­sticas Europa",
        },
    },
    "EU ê·œì œ ë° ì¤€ìˆ˜": {
        "emoji": "âš–ï¸",
        "queries": {
            "en": "EU AI Act CRA regulation electronics 2024",
            "de": "EU KI Gesetz CRA",
            "fr": "Loi IA UE CRA",
            "es": "Ley IA UE CRA",
        },
    },
    "í˜ì‹  ë° ìƒíƒœê³„": {
        "emoji": "ğŸš€",
        "queries": {
            "en": "European startups 6G robotics AI innovation 2024",
            "de": "EuropÃ¤ische Startups 6G Robotik",
            "fr": "startups europÃ©ens 6G robotique",
            "es": "startups europeos 6G robÃ³tica",
        },
    },
    "Samsung í¬íŠ¸í´ë¦¬ì˜¤": {
        "emoji": "ğŸ“±",
        "queries": {
            "en": "Samsung Europe technology innovation 2024",
            "de": "Samsung Europa Technologie",
            "fr": "Samsung Europe technologie",
            "es": "Samsung Europa tecnologÃ­a",
        },
    }
}

MAX_TOTAL_ARTICLES = 10
MAX_PER_CATEGORY = 2
HISTORY_FILE = "article_history.json"

# ===== HISTORY MANAGEMENT =====
def load_history():
    if not os.path.exists(HISTORY_FILE):
        return {"articles": {}, "content_hashes": set()}
    try:
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            data = json.load(f)
            data["content_hashes"] = set(data.get("content_hashes", []))
            return data
    except:
        return {"articles": {}, "content_hashes": set()}

def save_history(history):
    save_data = history.copy()
    save_data["content_hashes"] = list(history["content_hashes"])
    with open(HISTORY_FILE, "w", encoding="utf-8") as f:
        json.dump(save_data, f, ensure_ascii=False, indent=2)

def get_content_hash(title, content):
    text = f"{title}{content}".lower()
    text = re.sub(r'\s+', ' ', text)
    return hashlib.md5(text.encode()).hexdigest()

def is_duplicate(title, content, history):
    return get_content_hash(title, content) in history["content_hashes"]

def add_to_history(url, title, content, category):
    history = load_history()
    hash_val = get_content_hash(title, content)
    history["articles"][url] = {
        "title": title,
        "category": category,
        "date": datetime.now().isoformat()
    }
    history["content_hashes"].add(hash_val)
    save_history(history)

# ===== TRANSLATION =====
@st.cache_data
def translate_to_korean(text):
    try:
        from google_trans_new import google_translator
        translator = google_translator()
        return translator.translate(text, lang_src='en', lang_tgt='ko')
    except:
        return text

# ===== INTELLIGENT SUMMARIZATION =====
def extract_smart_summary(title, content):
    """
    Extract 3 key points from article content
    Format: - Point (with numbers/facts)
            Â· Detail explanation
    """
    
    # Clean content
    content = content.replace('\n', ' ').replace('\r', ' ')
    content = re.sub(r'\s+', ' ', content).strip()
    
    # Split by sentences
    sentences = re.split(r'(?<=[.!?])\s+', content)
    sentences = [s.strip() for s in sentences if len(s.strip()) > 20]
    
    # Score sentences
    def score_sentence(sent):
        score = 0
        # Prefer sentences with numbers/percentages
        if re.search(r'\d+[%]?', sent):
            score += 5
        # Prefer sentences about growth/change
        keywords = ['grow', 'increase', 'rise', 'jump', 'expand', 'reach', 'launch', 'announce', 'strike', 'disruption', 'regulation', 'innovation', 'market', 'chip', 'semiconductor']
        for kw in keywords:
            if kw.lower() in sent.lower():
                score += 3
        # Prefer longer sentences with more info
        if len(sent.split()) > 8:
            score += 2
        return score
    
    # Score and sort
    scored = [(sent, score_sentence(sent)) for sent in sentences]
    scored = sorted(scored, key=lambda x: x[1], reverse=True)
    
    # Get top 3
    top_3 = [sent for sent, _ in scored[:3]]
    
    # Return top 3 or default
    if len(top_3) < 3:
        top_3.extend([
            "ê¸°ì‚¬ì—ì„œ ì¶”ì¶œí•œ ì£¼ìš” ì •ë³´ì…ë‹ˆë‹¤.",
            "ì‹œì¥ ë™í–¥ ë° ë³€í™”ë¥¼ ë°˜ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
            "ë” ìì„¸í•œ ë‚´ìš©ì€ ì „ì²´ ê¸°ì‚¬ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        ])
    
    return top_3[:3]

# ===== SEARCH =====
def perform_search(category_config, category_name, tavily_client, history):
    all_results = []
    seen_urls = set()
    
    for lang_name, lang_code in LANGUAGES.items():
        if len(all_results) >= MAX_PER_CATEGORY:
            break
        
        query = category_config["queries"].get(lang_code, category_config["queries"]["en"])
        
        try:
            results = tavily_client.search(
                query=query,
                search_depth="advanced",
                max_results=3,
                include_raw_content=True
            )
            
            for res in results.get('results', []):
                if len(all_results) >= MAX_PER_CATEGORY:
                    break
                
                url = res.get('url')
                title = res.get('title', '')
                content = res.get('content', '')
                
                if not url or not content:
                    continue
                
                if url in seen_urls or url in history["articles"]:
                    continue
                
                if len(content) < 100:
                    continue
                
                if is_duplicate(title, content, history):
                    continue
                
                seen_urls.add(url)
                all_results.append({
                    "url": url,
                    "title": title,
                    "content": content,
                    "language": lang_name,
                })
        except:
            pass
    
    return all_results

# ===== MAIN UI =====
st.markdown("""
<div class="header-container">
    <h1>ğŸ›¡ï¸ Samsung ìœ ëŸ½ ì¡°ë‹¬ ì„¼í„° ì „ëµ ì¸í…”ë¦¬ì „ìŠ¤</h1>
    <p>ì „ëµ ì •ë³´ ëŒ€ì‹œë³´ë“œ â€¢ ì¼ì¼ ìë™í™” ë¦¬í¬íŠ¸</p>
</div>
""", unsafe_allow_html=True)

# Sidebar
st.sidebar.header("âš™ï¸ ì„¤ì •")
tavily_key = st.sidebar.text_input("Tavily API Key", type="password")

history = load_history()
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ“Š íˆìŠ¤í† ë¦¬")
col1, col2 = st.sidebar.columns(2)
col1.metric("ì¶”ì  ê¸°ì‚¬", len(history["articles"]))
col2.metric("ê³ ìœ  ì½˜í…ì¸ ", len(history["content_hashes"]))

if st.sidebar.button("ğŸ—‘ï¸ ì´ˆê¸°í™”", use_container_width=True):
    if os.path.exists(HISTORY_FILE):
        os.remove(HISTORY_FILE)
    st.rerun()

# Main
st.markdown("---")
run_report = st.button("ğŸš€ ë¦¬í¬íŠ¸ ìƒì„±", use_container_width=True)

st.markdown("---")

if run_report:
    if not tavily_key:
        st.error("âŒ Tavily API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    else:
        client = TavilyClient(api_key=tavily_key)
        history = load_history()
        
        pbar = st.progress(0)
        status = st.empty()
        
        all_articles = []
        by_category = {}
        
        for idx, (cat_name, cat_config) in enumerate(CATEGORIES.items()):
            status.text(f"ğŸ” {cat_name} ê²€ìƒ‰ ì¤‘...")
            
            results = perform_search(cat_config, cat_name, client, history)
            
            if results:
                by_category[cat_name] = results
                all_articles.extend(results)
            
            pbar.progress((idx + 1) / len(CATEGORIES))
        
        all_articles = all_articles[:MAX_TOTAL_ARTICLES]
        pbar.empty()
        status.empty()
        
        # Stats
        st.markdown("---")
        col1, col2, col3 = st.columns(3)
        col1.metric("ìƒˆ ê¸°ì‚¬", len(all_articles))
        col2.metric("ì¹´í…Œê³ ë¦¬", len(by_category))
        col3.metric("ì´ ê¸°ì‚¬", len(history["articles"]))
        
        st.markdown("---")
        
        if all_articles:
            article_num = 0
            
            for cat_name, articles in by_category.items():
                if article_num >= MAX_TOTAL_ARTICLES:
                    break
                
                emoji = CATEGORIES[cat_name]["emoji"]
                st.markdown(f"### {emoji} {cat_name}")
                st.markdown(f"*{len(articles)}ê°œì˜ ìƒˆë¡œìš´ ê¸°ì‚¬*")
                
                for article in articles:
                    if article_num >= MAX_TOTAL_ARTICLES:
                        break
                    
                    article_num += 1
                    
                    # Extract summary
                    with st.spinner(f"ğŸ“ ê¸°ì‚¬ {article_num} ë¶„ì„ ì¤‘..."):
                        summary_points = extract_smart_summary(article['title'], article['content'])
                        
                        try:
                            title_kr = translate_to_korean(article['title'])
                        except:
                            title_kr = article['title']
                    
                    # Display article
                    st.markdown(f"#### ğŸ“° {article_num}. {title_kr}")
                    col_a, col_b = st.columns([2, 1])
                    with col_a:
                        st.caption(f"ğŸŒ {article['language']}")
                    with col_b:
                        st.caption(f"ğŸ“‚ {cat_name}")
                    
                    # Display summary
                    st.markdown("**â–¡**")
                    st.markdown(f"- {summary_points[0]}")
                    st.markdown(f"  Â· ì£¼ìš” ë‚´ìš©")
                    st.markdown(f"- {summary_points[1]}")
                    st.markdown(f"  Â· ì¶”ê°€ ì •ë³´")
                    st.markdown(f"- {summary_points[2]}")
                    st.markdown(f"  Â· ìƒì„¸ ë‚´ìš©")
                    
                    # Buttons
                    col1, col2, col3 = st.columns([2, 1, 1])
                    with col1:
                        st.markdown(f"[ğŸ“– ì „ì²´ ê¸°ì‚¬]({article['url']})")
                    with col2:
                        if st.button("âœ… ì½ìŒ", key=f"r_{article_num}", use_container_width=True):
                            add_to_history(article['url'], article['title'], article['content'], cat_name)
                            st.success("ì™„ë£Œ!")
                    with col3:
                        if st.button("ğŸ”— ë§í¬", key=f"l_{article_num}", use_container_width=True):
                            st.code(article['url'])
                    
                    st.divider()
        
        else:
            st.warning("âš ï¸ ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
