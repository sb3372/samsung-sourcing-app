import requests
from bs4 import BeautifulSoup
import time
from typing import List, Dict
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WebCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        }
        self.timeout = 10
        self.processed_urls = set()
        self.url_lock = threading.Lock()
        
        # ì œì™¸í•  í‚¤ì›Œë“œ (AI/LLM/ì¼ë°˜ ê¸°ìˆ )
        self.exclude_keywords = [
            'ai', 'artificial intelligence', 'llm', 'chatgpt', 'openai',
            'machine learning', 'deep learning', 'neural', 'algorithm',
            'software', 'cloud', 'data center', 'server',
            'cryptocurrency', 'blockchain', 'crypto',
            'startup', 'investment', 'funding', 'venture',
        ]
        
        # í¬í•¨í•  í‚¤ì›Œë“œ (10ê°œ ì¹´í…Œê³ ë¦¬ ê´€ë ¨)
        self.include_keywords = [
            # Semiconductors
            'semiconductor', 'chip', 'processor', 'fab', 'foundry', 'tsmc', 'samsung', 'intel',
            'processor', 'cpu', 'gpu', 'asic', '5nm', '3nm',
            # Components
            'sensor', 'display', 'lcd', 'oled', 'capacitor', 'resistor',
            # Consumer Electronics
            'smartphone', 'iphone', 'android', 'tablet', 'smartwatch', 'wearable',
            # Energy/Power
            'battery', 'power', 'energy', 'charging', 'electric vehicle', 'ev',
            # Connectivity
            '5g', '6g', 'network', 'wifi', 'broadband', 'telecom',
            # Robotics
            'robot', 'automation', 'manufacturing',
            # Photonics
            'photon', 'quantum', 'laser', 'optical',
            # Materials
            'graphene', 'nanotechnology', 'material',
            # Raw Materials
            'rare earth', 'lithium', 'cobalt', 'mineral',
            # Sustainable
            'recycling', 'e-waste', 'circular economy', 'sustainability',
        ]
    
    def is_valid_article(self, title: str) -> bool:
        """
        ìœ íš¨í•œ ê¸°ì‚¬ì¸ì§€ í™•ì¸
        1. AI/LLM í‚¤ì›Œë“œ ì œì™¸
        2. 10ê°œ ì¹´í…Œê³ ë¦¬ ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨
        """
        text = title.lower()
        
        # 1. ì œì™¸ í‚¤ì›Œë“œ í™•ì¸
        for keyword in self.exclude_keywords:
            if keyword in text:
                logger.info(f"â­ï¸ ì œì™¸: {title[:50]}... (í‚¤ì›Œë“œ: {keyword})")
                return False
        
        # 2. í¬í•¨ í‚¤ì›Œë“œ í™•ì¸
        for keyword in self.include_keywords:
            if keyword in text:
                logger.info(f"âœ… í¬í•¨: {title[:50]}... (í‚¤ì›Œë“œ: {keyword})")
                return True
        
        logger.info(f"â­ï¸ ì œì™¸: {title[:50]}... (ê´€ë ¨ í‚¤ì›Œë“œ ì—†ìŒ)")
        return False
    
    def crawl_website(self, website_config: Dict) -> List[Dict]:
        """ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ ì¶”ì¶œ"""
        try:
            logger.info(f"ğŸ”— í¬ë¡¤ë§ ì‹œì‘: {website_config['name']}")
            
            response = requests.get(
                website_config['news_page'],
                headers=self.headers,
                timeout=self.timeout
            )
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                logger.warning(f"âŒ {website_config['name']}: HTTP {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = []
            
            # ê¸°ì‚¬ ìš”ì†Œ ì°¾ê¸°
            article_elements = soup.select(website_config['article_selector'])
            logger.info(f"ğŸ“° '{website_config['article_selector']}': {len(article_elements)}ê°œ")
            
            if not article_elements:
                fallback_selectors = [
                    "div.news-item", "div.story", "li.news", "div.article",
                    "article", "div[class*='article']", "div[class*='news']"
                ]
                
                for selector in fallback_selectors:
                    article_elements = soup.select(selector)
                    if len(article_elements) > 3:
                        logger.info(f"ğŸ“° ëŒ€ì²´ selector '{selector}': {len(article_elements)}ê°œ")
                        break
            
            if not article_elements:
                logger.warning(f"âš ï¸ {website_config['name']}: ê¸°ì‚¬ ìš”ì†Œ ì—†ìŒ")
                return []
            
            # ê° ê¸°ì‚¬ ì¶”ì¶œ
            for article_elem in article_elements[:100]:
                try:
                    # ì œëª©
                    title = None
                    title_elem = article_elem.select_one(website_config['title_selector'])
                    
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                    else:
                        for tag in article_elem.select("a"):
                            text = tag.get_text(strip=True)
                            if len(text) > 10:
                                title = text
                                break
                        
                        if not title:
                            for tag in article_elem.select("h2, h3, h1"):
                                text = tag.get_text(strip=True)
                                if len(text) > 10:
                                    title = text
                                    break
                    
                    if not title or len(title) < 10:
                        continue
                    
                    # ğŸ”’ ìœ íš¨í•œ ê¸°ì‚¬ì¸ì§€ í™•ì¸ (AI/LLM ì œì™¸, 10ê°œ ì¹´í…Œê³ ë¦¬ë§Œ)
                    if not self.is_valid_article(title):
                        continue
                    
                    # ë§í¬
                    link = None
                    link_elem = article_elem.select_one(website_config['link_selector'])
                    
                    if link_elem and link_elem.get('href'):
                        link = link_elem.get('href')
                    else:
                        for tag in article_elem.select("a"):
                            if tag.get('href'):
                                link = tag.get('href')
                                break
                    
                    if not link:
                        continue
                    
                    # URL ì²˜ë¦¬
                    if link.startswith('/'):
                        base_url = website_config['url'].rstrip('/')
                        link = base_url + link
                    elif not link.startswith('http'):
                        base_url = website_config['url'].rstrip('/')
                        link = base_url + '/' + link
                    
                    # ì¤‘ë³µ í™•ì¸
                    with self.url_lock:
                        if link in self.processed_urls:
                            continue
                        self.processed_urls.add(link)
                    
                    # ì €ì¥ (categoriesëŠ” configì—ì„œ ê°€ì ¸ì˜´)
                    article_data = {
                        'title_en': title,
                        'link': link,
                        'source': website_config['name'],
                        'categories': website_config['categories'],
                        'crawled_at': datetime.now().isoformat(),
                    }
                    
                    articles.append(article_data)
                
                except Exception as e:
                    logger.debug(f"âš ï¸ ì˜¤ë¥˜: {str(e)[:50]}")
                    continue
            
            logger.info(f"âœ… {website_config['name']}: {len(articles)}ê°œ\n")
            return articles
        
        except Exception as e:
            logger.error(f"âŒ {website_config['name']}: {str(e)[:100]}")
            return []
    
    def crawl_all_websites(self, websites: List[Dict], max_workers: int = 10) -> List[Dict]:
        """ëª¨ë“  ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘"""
        all_articles = []
        
        logger.info(f"ğŸš€ ì´ {len(websites)}ê°œ ì›¹ì‚¬ì´íŠ¸ ë³‘ë ¬ í¬ë¡¤ë§\n")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_website = {
                executor.submit(self.crawl_website, website): website 
                for website in websites
            }
            
            for future in as_completed(future_to_website):
                try:
                    articles = future.result()
                    all_articles.extend(articles)
                except Exception as e:
                    logger.error(f"âŒ ì˜¤ë¥˜: {str(e)}")
        
        logger.info(f"ğŸ“Š ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘\n")
        return all_articles
