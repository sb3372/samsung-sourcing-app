import requests
from bs4 import BeautifulSoup
import time
from typing import List, Dict, Optional
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WebCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        }
        self.timeout = 10
        self.processed_urls = set()
    
    def crawl_website(self, website_config: Dict) -> List[Dict]:
        """
        ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ ì¶”ì¶œ
        """
        try:
            logger.info(f"ğŸ”— í¬ë¡¤ë§ ì‹œì‘: {website_config['name']}")
            
            # ì›¹ì‚¬ì´íŠ¸ ì ‘ê·¼
            response = requests.get(
                website_config['news_page'],
                headers=self.headers,
                timeout=self.timeout
            )
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                logger.warning(f"âŒ {website_config['name']}: HTTP {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = []
            
            # ê¸°ì‚¬ ì»¨í…Œì´ë„ˆ ì°¾ê¸°
            article_elements = soup.select(website_config['article_selector'])
            logger.info(f"ğŸ“° {len(article_elements)}ê°œ ê¸°ì‚¬ ìš”ì†Œ ë°œê²¬")
            
            if not article_elements:
                logger.warning(f"âš ï¸ {website_config['name']}: ê¸°ì‚¬ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            # ê° ê¸°ì‚¬ ì¶”ì¶œ
            for idx, article_elem in enumerate(article_elements[:20]):
                try:
                    # ì œëª© ì¶”ì¶œ
                    title_elem = article_elem.select_one(website_config['title_selector'])
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    if not title or len(title) < 10:
                        continue
                    
                    # ë§í¬ ì¶”ì¶œ
                    link_elem = article_elem.select_one(website_config['link_selector'])
                    if not link_elem:
                        continue
                    
                    link = link_elem.get('href', '')
                    if not link:
                        continue
                    
                    # ìƒëŒ€ URL ì²˜ë¦¬
                    if link.startswith('/'):
                        base_url = website_config['url'].rstrip('/')
                        link = base_url + link
                    elif not link.startswith('http'):
                        base_url = website_config['url'].rstrip('/')
                        link = base_url + '/' + link
                    
                    # ì¤‘ë³µ í™•ì¸
                    if link in self.processed_urls:
                        continue
                    
                    self.processed_urls.add(link)
                    
                    # ê¸°ì‚¬ ì •ë³´ ì €ì¥
                    article_data = {
                        'title_en': title,
                        'link': link,
                        'source': website_config['name'],
                        'categories': website_config['categories'],
                        'crawled_at': datetime.now().isoformat(),
                    }
                    
                    articles.append(article_data)
                    logger.info(f"âœ… ê¸°ì‚¬ ì¶”ì¶œ: {title[:60]}...")
                
                except Exception as e:
                    logger.warning(f"âš ï¸ ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)[:50]}")
                    continue
            
            logger.info(f"âœ… {website_config['name']}: {len(articles)}ê°œ ê¸°ì‚¬ ì¶”ì¶œ ì™„ë£Œ\n")
            return articles
        
        except requests.Timeout:
            logger.error(f"â±ï¸ {website_config['name']}: íƒ€ì„ì•„ì›ƒ")
            return []
        except Exception as e:
            logger.error(f"âŒ {website_config['name']}: {str(e)[:100]}")
            return []
    
    def crawl_all_websites(self, websites: List[Dict], max_workers: int = 10) -> List[Dict]:
        """
        ëª¨ë“  ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬)
        
        Args:
            websites: config.pyì˜ WEBSITES ë¦¬ìŠ¤íŠ¸
            max_workers: ë™ì‹œì— ì²˜ë¦¬í•  ì›¹ì‚¬ì´íŠ¸ ê°œìˆ˜ (ê¸°ë³¸ 10ê°œ)
            
        Returns:
            ëª¨ë“  ê¸°ì‚¬ í†µí•© ë¦¬ìŠ¤íŠ¸
        """
        all_articles = []
        
        logger.info(f"ğŸš€ ì´ {len(websites)}ê°œ ì›¹ì‚¬ì´íŠ¸ ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘ (ë™ì‹œ {max_workers}ê°œ)\n")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # ëª¨ë“  ì›¹ì‚¬ì´íŠ¸ ì‘ì—… ì œì¶œ
            future_to_website = {
                executor.submit(self.crawl_website, website): website 
                for website in websites
            }
            
            # ì™„ë£Œëœ ì‘ì—…ë¶€í„° ì²˜ë¦¬
            for future in as_completed(future_to_website):
                website = future_to_website[future]
                try:
                    articles = future.result()
                    all_articles.extend(articles)
                except Exception as e:
                    logger.error(f"âŒ {website['name']}: {str(e)}")
        
        logger.info(f"ğŸ“Š ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ\n")
        return all_articles
