import requests
from bs4 import BeautifulSoup
import time
from typing import List, Dict, Optional
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed
import threading

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class WebCrawler:
    def __init__(self):
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
        }
        self.timeout = 10
        self.processed_urls = set()
        self.url_lock = threading.Lock()
    
    def crawl_website(self, website_config: Dict) -> List[Dict]:
        """
        ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ ì¶”ì¶œ (ì—¬ëŸ¬ selector ì‹œë„)
        """
        try:
            logger.info(f"ğŸ”— í¬ë¡¤ë§ ì‹œì‘: {website_config['name']}")
            
            response = requests.get(
                website_config['news_page'],
                headers=self.headers,
                timeout=self.timeout
            )
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                logger.warning(f"âŒ {website_config['name']}: HTTP {response.status_code}")
                return []
            
            soup = BeautifulSoup(response.text, 'html.parser')
            articles = []
            
            # 1ì°¨: ì„¤ì •ëœ selectorë¡œ ì‹œë„
            article_elements = soup.select(website_config['article_selector'])
            logger.info(f"ğŸ“° '{website_config['article_selector']}': {len(article_elements)}ê°œ")
            
            # 2ì°¨: ì‹¤íŒ¨í•˜ë©´ ë‹¤ë¥¸ selector ì‹œë„
            if not article_elements or len(article_elements) == 0:
                fallback_selectors = [
                    "div.news-item",
                    "div.story",
                    "li.news",
                    "div.article",
                    "article",
                    "div[class*='article']",
                    "div[class*='news']",
                    "div[class*='story']"
                ]
                
                for selector in fallback_selectors:
                    article_elements = soup.select(selector)
                    if len(article_elements) > 3:
                        logger.info(f"ğŸ“° ëŒ€ì²´ selector '{selector}': {len(article_elements)}ê°œ ë°œê²¬")
                        break
            
            if not article_elements or len(article_elements) < 1:
                logger.warning(f"âš ï¸ {website_config['name']}: ê¸°ì‚¬ ìš”ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
            
            # ê° ê¸°ì‚¬ ì¶”ì¶œ
            for idx, article_elem in enumerate(article_elements[:50]):
                try:
                    # ì œëª© ì°¾ê¸°
                    title = None
                    title_elem = article_elem.select_one(website_config['title_selector'])
                    
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                    else:
                        for tag in article_elem.select("a"):
                            text = tag.get_text(strip=True)
                            if len(text) > 10:
                                title = text
                                break
                        
                        if not title:
                            for tag in article_elem.select("h2, h3, h1"):
                                text = tag.get_text(strip=True)
                                if len(text) > 10:
                                    title = text
                                    break
                    
                    if not title or len(title) < 10:
                        continue
                    
                    # ë§í¬ ì°¾ê¸°
                    link = None
                    link_elem = article_elem.select_one(website_config['link_selector'])
                    
                    if link_elem and link_elem.get('href'):
                        link = link_elem.get('href')
                    else:
                        for tag in article_elem.select("a"):
                            if tag.get('href'):
                                link = tag.get('href')
                                break
                    
                    if not link:
                        continue
                    
                    # ìƒëŒ€ URL ì²˜ë¦¬
                    if link.startswith('/'):
                        base_url = website_config['url'].rstrip('/')
                        link = base_url + link
                    elif not link.startswith('http'):
                        base_url = website_config['url'].rstrip('/')
                        link = base_url + '/' + link
                    
                    # ì¤‘ë³µ í™•ì¸
                    with self.url_lock:
                        if link in self.processed_urls:
                            continue
                        self.processed_urls.add(link)
                    
                    # ê¸°ì‚¬ ì •ë³´ ì €ì¥ (categories ì—†ìŒ)
                    article_data = {
                        'title_en': title,
                        'link': link,
                        'source': website_config['name'],
                        'crawled_at': datetime.now().isoformat(),
                    }
                    
                    articles.append(article_data)
                    logger.info(f"âœ… ê¸°ì‚¬ ì¶”ì¶œ: {title[:60]}...")
                
                except Exception as e:
                    logger.debug(f"âš ï¸ ê¸°ì‚¬ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)[:50]}")
                    continue
            
            logger.info(f"âœ… {website_config['name']}: {len(articles)}ê°œ ê¸°ì‚¬ ì¶”ì¶œ ì™„ë£Œ\n")
            return articles
        
        except requests.Timeout:
            logger.error(f"â±ï¸ {website_config['name']}: íƒ€ì„ì•„ì›ƒ")
            return []
        except Exception as e:
            logger.error(f"âŒ {website_config['name']}: {str(e)[:100]}")
            return []
    
    def crawl_all_websites(self, websites: List[Dict], max_workers: int = 10) -> List[Dict]:
        """
        ëª¨ë“  ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ í¬ë¡¤ë§ (ë³‘ë ¬ ì²˜ë¦¬)
        """
        all_articles = []
        
        logger.info(f"ğŸš€ ì´ {len(websites)}ê°œ ì›¹ì‚¬ì´íŠ¸ ë³‘ë ¬ í¬ë¡¤ë§ ì‹œì‘ (ë™ì‹œ {max_workers}ê°œ)\n")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_website = {
                executor.submit(self.crawl_website, website): website 
                for website in websites
            }
            
            for future in as_completed(future_to_website):
                website = future_to_website[future]
                try:
                    articles = future.result()
                    all_articles.extend(articles)
                    if len(articles) > 0:
                        logger.info(f"âœ… {website['name']}: {len(articles)}ê°œ ê¸°ì‚¬ ì¶”ê°€")
                except Exception as e:
                    logger.error(f"âŒ {website['name']}: {str(e)}")
        
        logger.info(f"ğŸ“Š ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ\n")
        return all_articles
