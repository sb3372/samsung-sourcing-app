import feedparser
import requests
from bs4 import BeautifulSoup
import google.generativeai as genai
from datetime import datetime
import time
from typing import List, Dict, Optional
import difflib

class NewsScraper:
    def __init__(self, gemini_api_key: str, system_prompt: str):
        self.gemini_api_key = gemini_api_key
        self.system_prompt = system_prompt
        genai.configure(api_key=gemini_api_key)
        self.model = genai.GenerativeModel("gemini-pro")
        self.processed_urls = set()
        self.processed_titles = []
        
    def fetch_rss_feed(self, search_query: str, lang: str, ceid: str) -> List[Dict]:
        """Google News RSSì—ì„œ ê¸°ì‚¬ ê°€ì ¸ì˜¤ê¸°"""
        try:
            encoded_query = requests.utils.quote(search_query)
            
            # ìƒˆë¡œìš´ RSS URL í˜•ì‹
            rss_url = f"https://news.google.com/rss/search?q={encoded_query}&hl={lang}&gl={ceid.split(':')[0].lower()}"
            
            print(f"ğŸ“¡ ì‹œë„: {rss_url[:80]}...")
            
            feed = feedparser.parse(rss_url)
            articles = []
            
            if not feed.entries:
                print(f"âš ï¸ ê²°ê³¼ ì—†ìŒ: {search_query[:50]}")
                return []
            
            for entry in feed.entries[:10]:
                title = entry.get("title", "")
                link = entry.get("link", "")
                
                if not link:
                    continue
                
                # URL ì¤‘ë³µ í™•ì¸
                if link in self.processed_urls:
                    continue
                
                # ì œëª© ìœ ì‚¬ë„ í™•ì¸ (85% ì´ìƒ = ì¤‘ë³µ)
                if self.is_duplicate_title(title):
                    continue
                
                article_data = {
                    "title": title,
                    "link": link,
                    "source": entry.get("source", {}).get("title", "Unknown"),
                    "published": entry.get("published", ""),
                    "language": lang,
                    "region": ceid.split(":")[0],
                }
                
                articles.append(article_data)
                self.processed_titles.append(title)
                    
            return articles
        except Exception as e:
            print(f"âŒ RSS ì˜¤ë¥˜: {e}")
            return []
    
    def is_duplicate_title(self, title: str, threshold: float = 0.85) -> bool:
        """ì œëª© ìœ ì‚¬ë„ë¡œ ì¤‘ë³µ ê²€ì‚¬ (85% ì´ìƒ = ì¤‘ë³µ)"""
        for existing_title in self.processed_titles:
            similarity = difflib.SequenceMatcher(None, title.lower(), existing_title.lower()).ratio()
            if similarity >= threshold:
                return True
        return False
    
    def scrape_article(self, url: str) -> Optional[str]:
        """ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ - 5ê°€ì§€ ë°©ë²• ì‚¬ìš©"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept-Language': 'en-US,en;q=0.9',
                'Accept-Encoding': 'gzip, deflate',
            }
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ë¶ˆí•„ìš”í•œ ìš”ì†Œ ì œê±°
            for element in soup(["script", "style", "nav", "footer", "header", "aside", "noscript", "meta"]):
                element.decompose()
            
            text = ""
            
            # ë°©ë²• 1: article íƒœê·¸
            article = soup.find('article')
            if article:
                text = article.get_text()
                if len(text) > 200:
                    pass  # ì„±ê³µ
                else:
                    text = ""
            
            # ë°©ë²• 2: main íƒœê·¸
            if not text or len(text) < 200:
                main = soup.find('main')
                if main:
                    text = main.get_text()
                    if len(text) > 200:
                        pass  # ì„±ê³µ
                    else:
                        text = ""
            
            # ë°©ë²• 3: div classì—ì„œ content ì°¾ê¸°
            if not text or len(text) < 200:
                for div in soup.find_all('div', class_=lambda x: x and any(keyword in x.lower() for keyword in ['content', 'article', 'post', 'entry', 'body'])):
                    candidate_text = div.get_text()
                    if len(candidate_text) > 200:
                        text = candidate_text
                        break
            
            # ë°©ë²• 4: ëª¨ë“  p íƒœê·¸ ìˆ˜ì§‘
            if not text or len(text) < 200:
                paragraphs = soup.find_all('p')
                if paragraphs:
                    text = ' '.join([p.get_text(strip=True) for p in paragraphs])
            
            # ë°©ë²• 5: ì „ì²´ bodyì—ì„œ ì¶”ì¶œ
            if not text or len(text) < 200:
                body = soup.find('body')
                if body:
                    text = body.get_text()
                else:
                    text = soup.get_text()
            
            # í…ìŠ¤íŠ¸ ì •ì œ
            lines = (line.strip() for line in text.splitlines())
            chunks = (phrase.strip() for line in lines for phrase in line.split("  "))
            text = ' '.join(chunk for chunk in chunks if chunk)
            
            # ìµœì†Œ ê¸¸ì´ ì²´í¬
            if len(text) > 150:
                print(f"âœ… ìŠ¤í¬ë˜í•‘ ì„±ê³µ: {len(text)} ê¸€ì")
                return text[:3000]
            else:
                print(f"âŒ í…ìŠ¤íŠ¸ ë¶€ì¡±: {len(text)} ê¸€ì")
                return None
            
        except requests.Timeout:
            print(f"â±ï¸ íƒ€ì„ì•„ì›ƒ: {url[:50]}...")
            return None
        except Exception as e:
            print(f"âŒ ìŠ¤í¬ë˜í•‘ ì˜¤ë¥˜: {e}")
            return None
    
    def summarize_with_gemini(self, article_text: str) -> str:
        """Geminië¡œ í•œêµ­ì–´ ìš”ì•½"""
        try:
            message = self.model.generate_content(
                [self.system_prompt, "\n\nã€ê¸°ì‚¬ ë‚´ìš©ã€‘\n\n" + article_text],
                generation_config=genai.types.GenerationConfig(
                    max_output_tokens=600,
                    temperature=0.2,
                    top_p=0.8,
                )
            )
            
            result = message.text.strip()
            return result
            
        except Exception as e:
            print(f"âŒ Gemini ì˜¤ë¥˜: {e}")
            return f"ìš”ì•½_ì‹¤íŒ¨"
    
    def process_article(self, article: Dict) -> Optional[Dict]:
        """ê¸°ì‚¬ ì²˜ë¦¬ - ìŠ¤í¬ë˜í•‘ + Gemini ìš”ì•½"""
        # URL ì¤‘ë³µ í™•ì¸
        if article["link"] in self.processed_urls:
            print(f"â†º ì´ë¯¸ ì²˜ë¦¬ë¨: {article['title'][:50]}")
            return None
        
        # ê¸°ì‚¬ ì¶”ì¶œ
        print(f"ğŸ”— ì¶”ì¶œ ì‹œì‘: {article['title'][:60]}...")
        article_text = self.scrape_article(article["link"])
        if not article_text:
            print(f"âŒ ì¶”ì¶œ ì‹¤íŒ¨: {article['title'][:50]}")
            return None
        
        # Gemini ìš”ì•½
        print(f"ğŸ“ Gemini ìš”ì•½ ì¤‘...")
        summary = self.summarize_with_gemini(article_text)
        print(f"ğŸ“„ ìš”ì•½ ê²°ê³¼: {summary[:100]}...")
        
        # í•„í„°ë§ (ë” ê´€ëŒ€í•˜ê²Œ)
        if len(summary) < 50:
            print(f"âŒ ìš”ì•½ì´ ë„ˆë¬´ ì§§ìŒ ({len(summary)} ê¸€ì): {summary}")
            return None
        
        if "ìš”ì•½_ì‹¤íŒ¨" in summary:
            print(f"âŒ Gemini ìš”ì•½ ì‹¤íŒ¨")
            return None
        
        if "NOT_RELEVANT_TO_PROCUREMENT" in summary:
            print(f"â“˜ ê´€ë ¨ì„± ì—†ìŒ (ì¡°ë‹¬ê³¼ ë¬´ê´€): {article['title'][:50]}")
            return None
        
        if "INSUFFICIENT_DETAILS" in summary:
            print(f"â“˜ ìƒì„¸ ì •ë³´ ë¶€ì¡±: {article['title'][:50]}")
            return None
        
        # ì„±ê³µ!
        self.processed_urls.add(article["link"])
        print(f"âœ“ ê¸°ì‚¬ ì¶”ê°€ ì™„ë£Œ!")
        
        return {
            "title": article["title"],
            "link": article["link"],
            "source": article["source"],
            "published": article["published"],
            "language": article["language"],
            "region": article["region"],
            "summary": summary,
            "processed_at": datetime.now().isoformat(),
        }
