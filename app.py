import streamlit as st
import time
import logging
from config import WEBSITES, CATEGORIES
from crawler import WebCrawler
from deduplicator import Deduplicator
from categorizer import Categorizer
from collections import defaultdict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

st.set_page_config(page_title="Samsung Electronics Europe IPC", page_icon="ğŸ“±", layout="wide")

# ê°„ë‹¨í•œ ìŠ¤íƒ€ì¼
st.markdown("""
    <style>
    body { font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif; }
    .article-title { font-size: 1.2rem; font-weight: 600; color: #1e88e5; margin-bottom: 0.5rem; }
    .article-meta { font-size: 0.9rem; color: #666; margin-bottom: 0.8rem; }
    .article-source { background: #e3f2fd; padding: 0.3rem 0.8rem; border-radius: 4px; display: inline-block; margin-right: 0.5rem; }
    .article-category { background: #1e88e5; color: white; padding: 0.3rem 0.8rem; border-radius: 4px; display: inline-block; margin-right: 0.5rem; font-size: 0.85rem; }
    .divider { margin: 1.5rem 0; border-top: 1px solid #eee; }
    </style>
""", unsafe_allow_html=True)

if "articles" not in st.session_state:
    st.session_state.articles = []
if "deduplicator" not in st.session_state:
    st.session_state.deduplicator = Deduplicator()

# íƒ€ì´í‹€
st.title("ğŸ“± Samsung Electronics Europe IPC")
st.markdown("ìœ ëŸ½ ê¸°ìˆ  ë‰´ìŠ¤ - AI ê¸°ë°˜ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜")
st.divider()

# ì‚¬ì´ë“œë°” - ì„¤ì •ë§Œ
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    api_key = st.text_input("ğŸ”‘ Gemini API í‚¤", type="password")
    if api_key:
        st.session_state.gemini_key = api_key
        st.success("API ì—°ê²°ë¨")
    
    st.divider()
    
    st.subheader("ì¹´í…Œê³ ë¦¬ ì„ íƒ")
    selected_categories = []
    for category in CATEGORIES:
        if st.checkbox(category, value=True):
            selected_categories.append(category)
    
    st.session_state.selected_categories = selected_categories

# ë©”ì¸ - ë²„íŠ¼ë§Œ
if st.button("ğŸ”„ ê¸°ì‚¬ ë¡œë“œ", use_container_width=True, type="primary"):
    
    if "gemini_key" not in st.session_state:
        st.error("API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    elif not st.session_state.selected_categories:
        st.error("ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”")
    else:
        status = st.empty()
        
        try:
            # í¬ë¡¤ë§
            status.text("ğŸ”— ì›¹ì‚¬ì´íŠ¸ í¬ë¡¤ë§ ì¤‘...")
            crawler = WebCrawler()
            all_articles = crawler.crawl_all_websites(WEBSITES, max_workers=10)
            status.text(f"âœ… {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
            time.sleep(0.5)
            
            # ì¤‘ë³µ ì œê±°
            status.text("ğŸ” ì¤‘ë³µ ì œê±° ì¤‘...")
            unique_articles = []
            for article in all_articles:
                if not st.session_state.deduplicator.is_duplicate(article):
                    unique_articles.append(article)
            status.text(f"âœ… {len(unique_articles)}ê°œ ìƒˆ ê¸°ì‚¬")
            time.sleep(0.5)
            
            # AI ë¶„ë¥˜
            status.text("ğŸ¤– AI ë¶„ë¥˜ ì¤‘...")
            categorizer = Categorizer(st.session_state.gemini_key)
            categorized_articles = []
            for idx, article in enumerate(unique_articles):
                status.text(f"ğŸ¤– ë¶„ë¥˜ ì¤‘: {idx + 1}/{len(unique_articles)}")
                ai_categories = categorizer.categorize_article(article['title_en'])
                article['categories'] = ai_categories
                categorized_articles.append(article)
                time.sleep(0.2)
            status.text("âœ… ë¶„ë¥˜ ì™„ë£Œ")
            time.sleep(0.5)
            
            # í•„í„°ë§
            status.text("ğŸ“‚ í•„í„°ë§ ì¤‘...")
            filtered_articles = []
            for article in categorized_articles:
                if any(cat in article['categories'] for cat in st.session_state.selected_categories):
                    filtered_articles.append(article)
            status.text(f"âœ… {len(filtered_articles)}ê°œ ê¸°ì‚¬ í•„í„°ë§")
            time.sleep(0.5)
            
            # ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ 10ê°œ ì„ íƒ
            status.text("ğŸ¯ ê¸°ì‚¬ ì„ íƒ ì¤‘...")
            articles_by_source = defaultdict(list)
            for article in filtered_articles:
                articles_by_source[article['source']].append(article)
            
            final_articles = []
            source_index = defaultdict(int)
            
            while len(final_articles) < 10 and len(articles_by_source) > 0:
                for source in list(articles_by_source.keys()):
                    if len(final_articles) >= 10:
                        break
                    if source_index[source] < len(articles_by_source[source]):
                        article = articles_by_source[source][source_index[source]]
                        final_articles.append(article)
                        source_index[source] += 1
                
                if len(final_articles) < 10:
                    for source in list(articles_by_source.keys()):
                        if len(final_articles) >= 10:
                            break
                        source_index[source] = 0
            
            top_articles = final_articles[:10]
            
            # CSV ì €ì¥
            for article in top_articles:
                st.session_state.deduplicator.save_article({
                    'title_en': article['title_en'],
                    'link': article['link'],
                    'source': article['source'],
                    'categories': ','.join(article['categories'])
                })
            
            st.session_state.articles = top_articles
            status.empty()
            st.success(f"âœ… {len(top_articles)}ê°œ ê¸°ì‚¬ ë¡œë“œ ì™„ë£Œ!")
        
        except Exception as e:
            st.error(f"ì˜¤ë¥˜: {str(e)}")
            logger.error(f"ì˜¤ë¥˜: {str(e)}")

# ê¸°ì‚¬ í‘œì‹œ
st.divider()

if st.session_state.articles:
    st.subheader(f"ğŸ“° ê¸°ì‚¬ ({len(st.session_state.articles)}ê°œ)")
    
    for idx, article in enumerate(st.session_state.articles, 1):
        st.markdown(f'<div class="article-title">{idx}. {article["title_en"]}</div>', unsafe_allow_html=True)
        
        meta_html = f'<div class="article-meta">'
        meta_html += f'<span class="article-source">{article["source"]}</span>'
        for cat in article['categories']:
            meta_html += f'<span class="article-category">{cat}</span>'
        meta_html += '</div>'
        st.markdown(meta_html, unsafe_allow_html=True)
        
        st.markdown(f'[ğŸ”— ì›ë¬¸ ì½ê¸°]({article["link"]})')
        st.markdown('<div class="divider"></div>', unsafe_allow_html=True)

else:
    st.info("ê¸°ì‚¬ë¥¼ ë¡œë“œí•˜ë ¤ë©´ 'ê¸°ì‚¬ ë¡œë“œ' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”")
