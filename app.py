import streamlit as st
import time
import logging
from config import WEBSITES, CATEGORIES
from crawler import WebCrawler
from categorizer import Categorizer
from deduplicator import Deduplicator
from collections import defaultdict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

st.set_page_config(page_title="Samsung Electronics Europe IPC", page_icon="ğŸ“±", layout="wide")

st.markdown("""
    <style>
    .article-title { font-size: 1.2rem; font-weight: 600; color: #1e88e5; margin-bottom: 0.5rem; }
    .article-meta { font-size: 0.9rem; color: #666; margin-bottom: 0.8rem; }
    .article-source { background: #e3f2fd; padding: 0.3rem 0.8rem; border-radius: 4px; display: inline-block; margin-right: 0.5rem; }
    .article-category { background: #1e88e5; color: white; padding: 0.3rem 0.8rem; border-radius: 4px; display: inline-block; margin-right: 0.5rem; font-size: 0.85rem; }
    .divider { margin: 1.5rem 0; border-top: 1px solid #eee; }
    </style>
""", unsafe_allow_html=True)

if "cached_articles" not in st.session_state:
    st.session_state.cached_articles = []  # í¬ë¡¤ë§ ê²°ê³¼ ìºì‹œ
if "displayed_articles" not in st.session_state:
    st.session_state.displayed_articles = []  # í‘œì‹œí•  ê¸°ì‚¬
if "week_range" not in st.session_state:
    st.session_state.week_range = 1  # 1ì£¼ì¼, 2ì£¼ì¼, 3ì£¼ì¼...
if "deduplicator" not in st.session_state:
    st.session_state.deduplicator = Deduplicator()

st.title("ğŸ“± Samsung Electronics Europe IPC")
st.markdown("ìœ ëŸ½ ê¸°ìˆ  ë‰´ìŠ¤ - AI ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜")
st.divider()

with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    api_key = st.text_input("ğŸ”‘ Gemini API í‚¤", type="password")
    if api_key:
        st.session_state.gemini_key = api_key
        st.success("API ì—°ê²°ë¨")
    
    st.divider()
    
    st.subheader("ì¹´í…Œê³ ë¦¬ ì„ íƒ")
    selected_categories = []
    for category in CATEGORIES:
        if st.checkbox(category, value=True):
            selected_categories.append(category)
    
    st.session_state.selected_categories = selected_categories

col1, col2 = st.columns([3, 1])

with col1:
    st.header("ğŸ”„ ë‰´ìŠ¤ ìˆ˜ì§‘")

with col2:
    if st.button("ğŸ”„ ì²˜ìŒë¶€í„°", use_container_width=True):
        st.session_state.cached_articles = []
        st.session_state.displayed_articles = []
        st.session_state.week_range = 1
        st.rerun()

# í¬ë¡¤ë§ ë²„íŠ¼
if st.button("ğŸ“¥ ê¸°ì‚¬ ë¡œë“œ", use_container_width=True, type="primary"):
    
    if "gemini_key" not in st.session_state:
        st.error("API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    elif not st.session_state.selected_categories:
        st.error("ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”")
    else:
        status = st.empty()
        
        try:
            status.text(f"ğŸ”— {st.session_state.week_range}ì£¼ì¼ ê¸°ì‚¬ í¬ë¡¤ë§ ì¤‘...")
            crawler = WebCrawler()
            all_articles = crawler.crawl_all_websites(WEBSITES, max_workers=10)
            status.text(f"âœ… {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
            time.sleep(0.5)
            
            # ìºì‹œì— ì¶”ê°€ (ìƒˆ ê¸°ì‚¬ë§Œ)
            for article in all_articles:
                if not st.session_state.deduplicator.is_duplicate(article):
                    st.session_state.cached_articles.append(article)
            
            logger.info(f"ìºì‹œë¨: {len(st.session_state.cached_articles)}ê°œ")
            status.text(f"âœ… ìºì‹œë¨: {len(st.session_state.cached_articles)}ê°œ")
            time.sleep(0.5)
            
            # AI ë¶„ë¥˜
            status.text("ğŸ¤– AI ë¶„ë¥˜ ì¤‘...")
            categorizer = Categorizer(st.session_state.gemini_key)
            
            categorized_articles = []
            for idx, article in enumerate(st.session_state.cached_articles):
                if 'categories' not in article or not article['categories']:
                    status.text(f"ğŸ¤– ë¶„ë¥˜ ì¤‘: {idx + 1}/{len(st.session_state.cached_articles)}")
                    ai_categories = categorizer.categorize_article(article['title_en'])
                    article['categories'] = ai_categories
                    time.sleep(0.2)
                
                categorized_articles.append(article)
            
            st.session_state.cached_articles = categorized_articles
            status.text("âœ… ë¶„ë¥˜ ì™„ë£Œ")
            time.sleep(0.5)
            
            # í•„í„°ë§ (ì„ íƒí•œ ì¹´í…Œê³ ë¦¬ë§Œ)
            status.text("ğŸ“‚ í•„í„°ë§ ì¤‘...")
            filtered_articles = []
            for article in st.session_state.cached_articles:
                if any(cat in article.get('categories', []) for cat in st.session_state.selected_categories):
                    filtered_articles.append(article)
            
            status.text(f"âœ… {len(filtered_articles)}ê°œ ê¸°ì‚¬ í•„í„°ë§")
            time.sleep(0.5)
            
            # ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ 10ê°œ ì„ íƒ
            articles_by_source = defaultdict(list)
            for article in filtered_articles:
                articles_by_source[article['source']].append(article)
            
            final_articles = []
            source_index = defaultdict(int)
            
            while len(final_articles) < 10 and len(articles_by_source) > 0:
                for source in list(articles_by_source.keys()):
                    if len(final_articles) >= 10:
                        break
                    if source_index[source] < len(articles_by_source[source]):
                        article = articles_by_source[source][source_index[source]]
                        final_articles.append(article)
                        source_index[source] += 1
                
                if len(final_articles) < 10:
                    for source in list(articles_by_source.keys()):
                        source_index[source] = 0
            
            st.session_state.displayed_articles = final_articles[:10]
            
            # CSV ì €ì¥
            for article in st.session_state.displayed_articles:
                st.session_state.deduplicator.save_article({
                    'title_en': article['title_en'],
                    'link': article['link'],
                    'source': article['source'],
                    'categories': ','.join(article.get('categories', []))
                })
            
            status.empty()
            st.success(f"âœ… {len(st.session_state.displayed_articles)}ê°œ ê¸°ì‚¬ ì¤€ë¹„ ì™„ë£Œ!")
            
            # ë‹¤ìŒ ì£¼ì¼ í´ë¦­ ìœ ë„
            if len(st.session_state.cached_articles) < 50:
                st.info(f"ğŸ’¡ ê¸°ì‚¬ê°€ ë¶€ì¡±í•˜ë©´ 'ì£¼ì¼ í™•ì¥' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”")
        
        except Exception as e:
            st.error(f"ì˜¤ë¥˜: {str(e)}")
            logger.error(f"ì˜¤ë¥˜: {str(e)}")

# ì£¼ì¼ í™•ì¥ ë²„íŠ¼
if st.button("ğŸ“… ì£¼ì¼ í™•ì¥ (ë” ë§ì€ ê¸°ì‚¬)", use_container_width=True):
    st.session_state.week_range += 1
    st.info(f"ë‹¤ìŒ ì¡°íšŒëŠ” {st.session_state.week_range}ì£¼ì¼ ë²”ìœ„ë¡œ ì§„í–‰ë©ë‹ˆë‹¤")

st.divider()

# ê¸°ì‚¬ í‘œì‹œ
if st.session_state.displayed_articles:
    st.subheader(f"ğŸ“° ê¸°ì‚¬ ({len(st.session_state.displayed_articles)}ê°œ)")
    
    for idx, article in enumerate(st.session_state.displayed_articles, 1):
        st.markdown(f'<div class="article-title">{idx}. {article["title_en"]}</div>', unsafe_allow_html=True)
        
        meta_html = f'<div class="article-meta">'
        meta_html += f'<span class="article-source">{article["source"]}</span>'
        for cat in article.get('categories', []):
            meta_html += f'<span class="article-category">{cat}</span>'
        meta_html += '</div>'
        st.markdown(meta_html, unsafe_allow_html=True)
        
        st.markdown(f'[ğŸ”— ì›ë¬¸ ì½ê¸°]({article["link"]})')
        st.markdown('<div class="divider"></div>', unsafe_allow_html=True)

else:
    st.info("ê¸°ì‚¬ë¥¼ ë¡œë“œí•˜ë ¤ë©´ 'ê¸°ì‚¬ ë¡œë“œ' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”")
