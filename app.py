import streamlit as st
import time
import logging
from config import WEBSITES, CATEGORIES
from crawler import WebCrawler
from categorizer import Categorizer
from deduplicator import Deduplicator

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

st.set_page_config(page_title="Samsung Electronics Europe IPC", page_icon="ğŸ“±", layout="wide")

st.markdown("""
    <style>
    .article-title { font-size: 1.2rem; font-weight: 600; color: #1e88e5; margin-bottom: 0.5rem; }
    .article-meta { font-size: 0.9rem; color: #666; margin-bottom: 0.8rem; }
    .article-source { background: #e3f2fd; padding: 0.3rem 0.8rem; border-radius: 4px; display: inline-block; margin-right: 0.5rem; }
    .article-category { background: #1e88e5; color: white; padding: 0.3rem 0.8rem; border-radius: 4px; display: inline-block; margin-right: 0.5rem; font-size: 0.85rem; }
    .divider { margin: 1.5rem 0; border-top: 1px solid #eee; }
    </style>
""", unsafe_allow_html=True)

# ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
if "all_articles" not in st.session_state:
    st.session_state.all_articles = []  # í¬ë¡¤ë§ + ë¶„ë¥˜ëœ ëª¨ë“  ê¸°ì‚¬
if "current_page" not in st.session_state:
    st.session_state.current_page = 0
if "week_range" not in st.session_state:
    st.session_state.week_range = 1
if "deduplicator" not in st.session_state:
    st.session_state.deduplicator = Deduplicator()
if "last_crawled_week" not in st.session_state:
    st.session_state.last_crawled_week = 0

st.title("ğŸ“± Samsung Electronics Europe IPC")
st.markdown("ìœ ëŸ½ ê¸°ìˆ  ë‰´ìŠ¤ - AI ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜")
st.divider()

with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    api_key = st.text_input("ğŸ”‘ Gemini API í‚¤", type="password")
    if api_key:
        st.session_state.gemini_key = api_key
        st.success("API ì—°ê²°ë¨")
    
    st.divider()
    
    st.subheader("ì¹´í…Œê³ ë¦¬ ì„ íƒ")
    selected_categories = []
    for category in CATEGORIES:
        if st.checkbox(category, value=True):
            selected_categories.append(category)
    
    st.session_state.selected_categories = selected_categories

# ìƒíƒœ í‘œì‹œ
col1, col2, col3 = st.columns(3)
with col1:
    st.metric("ì „ì²´ ê¸°ì‚¬", len(st.session_state.all_articles))
with col2:
    st.metric("í˜„ì¬ í˜ì´ì§€", st.session_state.current_page + 1)
with col3:
    st.metric("ì¡°íšŒ ë²”ìœ„", f"{st.session_state.week_range}ì£¼ì¼")

st.divider()

# í¬ë¡¤ë§ ë²„íŠ¼
if st.button("ğŸ“¥ ì‹œì‘ (1ì£¼ì¼)", use_container_width=True, type="primary"):
    if "gemini_key" not in st.session_state:
        st.error("API í‚¤ë¥¼ ì…ë ¥í•˜ì„¸ìš”")
    elif not st.session_state.selected_categories:
        st.error("ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”")
    else:
        # ìƒˆë¡œìš´ ì£¼ì¼ ë²”ìœ„ë¡œ í¬ë¡¤ë§í•´ì•¼ í•  ë•Œë§Œ
        if st.session_state.last_crawled_week != st.session_state.week_range:
            status = st.empty()
            
            try:
                # 1ë‹¨ê³„: í¬ë¡¤ë§
                status.text(f"ğŸ”— {st.session_state.week_range}ì£¼ì¼ ê¸°ì‚¬ í¬ë¡¤ë§ ì¤‘...")
                crawler = WebCrawler()
                all_articles = crawler.crawl_all_websites(WEBSITES, max_workers=10)
                status.text(f"âœ… {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
                time.sleep(0.5)
                
                # 2ë‹¨ê³„: ì¤‘ë³µ ì œê±°
                status.text("ğŸ” ì¤‘ë³µ ì œê±° ì¤‘...")
                unique_articles = []
                for article in all_articles:
                    if not st.session_state.deduplicator.is_duplicate(article):
                        unique_articles.append(article)
                status.text(f"âœ… {len(unique_articles)}ê°œ ìƒˆ ê¸°ì‚¬")
                time.sleep(0.5)
                
                # 3ë‹¨ê³„: AI ë¶„ë¥˜
                status.text("ğŸ¤– AI ë¶„ë¥˜ ì¤‘...")
                categorizer = Categorizer(st.session_state.gemini_key)
                
                categorized_articles = []
                for idx, article in enumerate(unique_articles):
                    status.text(f"ğŸ¤– ë¶„ë¥˜ ì¤‘: {idx + 1}/{len(unique_articles)}")
                    ai_categories = categorizer.categorize_article(article['title_en'])
                    article['categories'] = ai_categories
                    categorized_articles.append(article)
                    time.sleep(0.1)
                
                status.text("âœ… ë¶„ë¥˜ ì™„ë£Œ")
                time.sleep(0.5)
                
                # 4ë‹¨ê³„: ì„ íƒ ì¹´í…Œê³ ë¦¬ í•„í„°ë§
                status.text("ğŸ“‚ í•„í„°ë§ ì¤‘...")
                filtered_articles = []
                for article in categorized_articles:
                    if any(cat in article.get('categories', []) for cat in st.session_state.selected_categories):
                        filtered_articles.append(article)
                
                status.text(f"âœ… {len(filtered_articles)}ê°œ ê¸°ì‚¬ í•„í„°ë§")
                time.sleep(0.5)
                
                st.session_state.all_articles = filtered_articles
                st.session_state.current_page = 0
                st.session_state.last_crawled_week = st.session_state.week_range
                
                status.empty()
                st.success(f"âœ… {len(filtered_articles)}ê°œ ê¸°ì‚¬ ì¤€ë¹„ ì™„ë£Œ!")
                st.rerun()
            
            except Exception as e:
                st.error(f"ì˜¤ë¥˜: {str(e)}")
                logger.error(f"ì˜¤ë¥˜: {str(e)}")

st.divider()

# ê¸°ì‚¬ í‘œì‹œ (10ê°œì”©)
if st.session_state.all_articles:
    start_idx = st.session_state.current_page * 10
    end_idx = start_idx + 10
    page_articles = st.session_state.all_articles[start_idx:end_idx]
    
    st.subheader(f"ğŸ“° ê¸°ì‚¬ (í˜ì´ì§€ {st.session_state.current_page + 1}/{(len(st.session_state.all_articles) + 9) // 10})")
    
    for idx, article in enumerate(page_articles, 1):
        st.markdown(f'<div class="article-title">{start_idx + idx}. {article["title_en"]}</div>', unsafe_allow_html=True)
        
        meta_html = f'<div class="article-meta">'
        meta_html += f'<span class="article-source">{article["source"]}</span>'
        for cat in article.get('categories', []):
            meta_html += f'<span class="article-category">{cat}</span>'
        meta_html += '</div>'
        st.markdown(meta_html, unsafe_allow_html=True)
        
        st.markdown(f'[ğŸ”— ì›ë¬¸ ì½ê¸°]({article["link"]})')
        st.markdown('<div class="divider"></div>', unsafe_allow_html=True)
        
        # CSV ì €ì¥
        st.session_state.deduplicator.save_article({
            'title_en': article['title_en'],
            'link': article['link'],
            'source': article['source'],
            'categories': ','.join(article.get('categories', []))
        })
    
    # í˜ì´ì§€ë„¤ì´ì…˜
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        if st.session_state.current_page > 0:
            if st.button("â¬…ï¸ ì´ì „ í˜ì´ì§€", use_container_width=True):
                st.session_state.current_page -= 1
                st.rerun()
    
    with col2:
        if end_idx < len(st.session_state.all_articles):
            if st.button("â¡ï¸ ë‹¤ìŒ í˜ì´ì§€", use_container_width=True):
                st.session_state.current_page += 1
                st.rerun()
    
    with col3:
        if end_idx >= len(st.session_state.all_articles) and st.session_state.week_range < 4:
            if st.button("ğŸ“… ì£¼ì¼ í™•ì¥", use_container_width=True):
                st.session_state.week_range += 1
                st.rerun()
    
    with col4:
        if st.button("ğŸ”„ ì²˜ìŒë¶€í„°", use_container_width=True):
            st.session_state.all_articles = []
            st.session_state.current_page = 0
            st.session_state.week_range = 1
            st.session_state.last_crawled_week = 0
            st.rerun()

else:
    st.info("ğŸ“¥ 'ì‹œì‘ (1ì£¼ì¼)' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ê¸°ì‚¬ë¥¼ ë¡œë“œí•˜ì„¸ìš”")
