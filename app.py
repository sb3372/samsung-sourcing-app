import streamlit as st
import time
import logging
from config import WEBSITES, CATEGORIES
from crawler import WebCrawler
from deduplicator import Deduplicator
from categorizer import Categorizer
from collections import defaultdict

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

st.set_page_config(page_title="Samsung ë‰´ìŠ¤", page_icon="ğŸ“°", layout="wide")

if "articles" not in st.session_state:
    st.session_state.articles = []
if "deduplicator" not in st.session_state:
    st.session_state.deduplicator = Deduplicator()

st.title("ğŸ“° Samsung êµ­ì œ ì¡°ë‹¬ì„¼í„°")
st.markdown("ìœ ëŸ½ ê¸°ìˆ  ë‰´ìŠ¤ - AI ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜")
st.markdown("---")

# ì‚¬ì´ë“œë°”
with st.sidebar:
    st.header("âš™ï¸ ì„¤ì •")
    
    # Gemini API í‚¤
    api_key = st.text_input(
        "ğŸ”‘ Gemini API í‚¤",
        type="password",
        help="https://aistudio.google.com/app/apikeyì—ì„œ ë°œê¸‰"
    )
    
    if api_key:
        st.session_state.gemini_key = api_key
        st.success("âœ… API ì¤€ë¹„ ì™„ë£Œ")
    
    st.markdown("---")
    
    st.header("ğŸ“‚ ì¹´í…Œê³ ë¦¬ ì„ íƒ")
    selected_categories = []
    
    for category in CATEGORIES:
        if st.checkbox(category, value=True):
            selected_categories.append(category)
    
    st.session_state.selected_categories = selected_categories

st.header("ğŸ”„ ë‰´ìŠ¤ ìˆ˜ì§‘")

if st.button("ğŸ”„ ìƒˆë¡œìš´ ê¸°ì‚¬ ë¡œë“œ", use_container_width=True, type="primary"):
    
    if "gemini_key" not in st.session_state:
        st.error("âŒ API í‚¤ë¥¼ ë¨¼ì € ì…ë ¥í•˜ì„¸ìš”")
    elif not st.session_state.selected_categories:
        st.error("âŒ ì¹´í…Œê³ ë¦¬ë¥¼ ì„ íƒí•˜ì„¸ìš”")
    else:
        status_text = st.empty()
        
        try:
            # 1ë‹¨ê³„: ë³‘ë ¬ ì›¹ í¬ë¡¤ë§
            status_text.text("ğŸ”— ì›¹ì‚¬ì´íŠ¸ ë³‘ë ¬ í¬ë¡¤ë§ ì¤‘...")
            
            crawler = WebCrawler()
            all_articles = crawler.crawl_all_websites(WEBSITES, max_workers=10)
            
            logger.info(f"ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘")
            status_text.text(f"âœ… {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
            time.sleep(1)
            
            # 2ë‹¨ê³„: ì¤‘ë³µ ì œê±°
            status_text.text("ğŸ” ì¤‘ë³µ ì œê±° ì¤‘...")
            unique_articles = []
            
            for article in all_articles:
                if not st.session_state.deduplicator.is_duplicate(article):
                    unique_articles.append(article)
            
            logger.info(f"ì¤‘ë³µ ì œê±° í›„ {len(unique_articles)}ê°œ ê¸°ì‚¬")
            status_text.text(f"âœ… {len(unique_articles)}ê°œ ìƒˆ ê¸°ì‚¬ ë°œê²¬")
            time.sleep(1)
            
            # 3ë‹¨ê³„: AIë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
            status_text.text("ğŸ¤– AI ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì¤‘...")
            categorizer = Categorizer(st.session_state.gemini_key)
            
            categorized_articles = []
            for idx, article in enumerate(unique_articles):
                status_text.text(f"ë¶„ë¥˜ ì¤‘: {idx + 1}/{len(unique_articles)}")
                
                # AIë¡œ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                ai_categories = categorizer.categorize_article(article['title_en'])
                article['categories'] = ai_categories
                categorized_articles.append(article)
                
                time.sleep(0.3)  # API ìš”ì²­ ê°„ê²©
            
            logger.info(f"ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì™„ë£Œ")
            status_text.text(f"âœ… ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ ì™„ë£Œ")
            time.sleep(1)
            
            # 4ë‹¨ê³„: ì„ íƒëœ ì¹´í…Œê³ ë¦¬ë¡œ í•„í„°ë§
            status_text.text("ğŸ“‚ ì¹´í…Œê³ ë¦¬ í•„í„°ë§ ì¤‘...")
            filtered_articles = []
            
            for article in categorized_articles:
                # ì„ íƒëœ ì¹´í…Œê³ ë¦¬ì™€ ê²¹ì¹˜ëŠ”ì§€ í™•ì¸
                if any(cat in article['categories'] for cat in st.session_state.selected_categories):
                    filtered_articles.append(article)
            
            logger.info(f"í•„í„°ë§ í›„ {len(filtered_articles)}ê°œ ê¸°ì‚¬")
            status_text.text(f"ğŸ“‚ {len(filtered_articles)}ê°œ ê¸°ì‚¬ í•„í„°ë§ ì™„ë£Œ")
            time.sleep(1)
            
            # 5ë‹¨ê³„: ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ 10ê°œ ì„ íƒ
            status_text.text("ğŸ¯ ë‹¤ì–‘í•œ ì†ŒìŠ¤ì—ì„œ ê¸°ì‚¬ ì„ íƒ ì¤‘...")
            
            # ì†ŒìŠ¤ë³„ë¡œ ê¸°ì‚¬ ë¶„ë¥˜
            articles_by_source = defaultdict(list)
            for article in filtered_articles:
                articles_by_source[article['source']].append(article)
            
            # ê° ì†ŒìŠ¤ì—ì„œ ê³ ë¥´ê²Œ ì„ íƒ
            final_articles = []
            source_index = defaultdict(int)
            
            while len(final_articles) < 10 and len(articles_by_source) > 0:
                for source in list(articles_by_source.keys()):
                    if len(final_articles) >= 10:
                        break
                    
                    if source_index[source] < len(articles_by_source[source]):
                        article = articles_by_source[source][source_index[source]]
                        final_articles.append(article)
                        source_index[source] += 1
                
                # ëª¨ë“  ì†ŒìŠ¤ë¥¼ í•œ ë²ˆ ìˆœíšŒí–ˆëŠ”ë°ë„ 10ê°œ ë¯¸ë§Œì´ë©´, ì¤‘ë³µ ì„ íƒ
                if len(final_articles) < 10:
                    for source in list(articles_by_source.keys()):
                        if len(final_articles) >= 10:
                            break
                        source_index[source] = 0  # ì´ˆê¸°í™”
            
            top_articles = final_articles[:10]
            
            # ê¸°ì‚¬ë¥¼ CSVì— ì €ì¥ (ì¤‘ë³µ ë“±ë¡)
            for article in top_articles:
                st.session_state.deduplicator.save_article({
                    'title_en': article['title_en'],
                    'link': article['link'],
                    'source': article['source'],
                    'categories': ','.join(article['categories'])
                })
            
            st.session_state.articles = top_articles
            
            time.sleep(1)
            status_text.empty()
            st.success(f"âœ… {len(top_articles)}ê°œ ê¸°ì‚¬ ë¡œë“œ ì™„ë£Œ!")
        
        except Exception as e:
            st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
            import traceback
            st.error(traceback.format_exc())
            logger.error(f"ì „ì²´ ì˜¤ë¥˜: {str(e)}")

st.markdown("---")

if st.session_state.articles:
    st.header(f"ğŸ“Š ìˆ˜ì§‘ëœ ê¸°ì‚¬ ({len(st.session_state.articles)}ê°œ)")
    
    for idx, article in enumerate(st.session_state.articles, 1):
        with st.container():
            st.subheader(f"{idx}. {article['title_en']}")
            
            col1, col2, col3 = st.columns(3)
            with col1:
                categories_str = ", ".join(article['categories'])
                st.caption(f"ğŸ“‚ {categories_str}")
            with col2:
                st.caption(f"ì¶œì²˜: {article['source']}")
            with col3:
                st.markdown(f"[ğŸ”— ì›ë¬¸]({article['link']})")
            
            st.divider()

else:
    st.info("ğŸ”„ 'ìƒˆë¡œìš´ ê¸°ì‚¬ ë¡œë“œ' ë²„íŠ¼ì„ í´ë¦­í•˜ì—¬ ê¸°ì‚¬ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”")
